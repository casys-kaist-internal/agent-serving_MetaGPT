export CUDA_VISIBLE_DEVICES=0,1

export NPROC_PER_NODE=$(echo $CUDA_VISIBLE_DEVICES | tr ',' '\n' | wc -l)

export OPENAI_API_KEY="token-abc123"

############### VLLM MODEL Setting ###############

# export VLLM_CONTEXT_LENGTH=2048
# export VLLM_MODEL_NAME="facebook/opt-125m"

# export VLLM_CONTEXT_LENGTH=32768
# export VLLM_MODEL_NAME="Qwen/Qwen2.5-7B-Instruct"

# export VLLM_CONTEXT_LENGTH=131072
# export VLLM_MODEL_NAME="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"

# export VLLM_CONTEXT_LENGTH=32768
# export VLLM_MODEL_NAME="Qwen/Qwen2.5-3B"

export VLLM_CONTEXT_LENGTH=32768
export VLLM_MODEL_NAME="Qwen/Qwen3-4B"

# export VLLM_MODEL_NAME="zai-org/GLM-4-9B-0414"

# export VLLM_CONTEXT_LENGTH=32768
# export VLLM_MODEL_NAME="Qwen/Qwen3-8B"

# export VLLM_CONTEXT_LENGTH=32768
# export VLLM_CONTEXT_LENGTH=131072
# export VLLM_MODEL_NAME="Qwen/Qwen3-14B"

# export VLLM_CONTEXT_LENGTH=32768
# export VLLM_MODEL_NAME="Qwen/Qwen2.5-1.5B-Instruct"

# export VLLM_CONTEXT_LENGTH=1000000
# export VLLM_MODEL_NAME="Qwen/Qwen2.5-7B-Instruct-1M"

# export VLLM_CONTEXT_LENGTH=131072
# export VLLM_MODEL_NAME="google/gemma-3-4b-it"

# export VLLM_CONTEXT_LENGTH=131072
# export VLLM_MODEL_NAME="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"